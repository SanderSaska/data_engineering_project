services:
  airflow:
    build:
      context: ./docker
      dockerfile: Dockerfile-airflow
    image: custom-airflow:latest
    container_name: airflow
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-metadata/airflow_metadata
    ports:
      - "8080:8080"
    depends_on:
      - airflow-metadata
      - postgres
      - oracle
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    networks:
      - airflow-net

  airflow-webserver:
    image: apache/airflow:2.6.3
    container_name: airflow-webserver
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-metadata/airflow_metadata
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    ports:
      - "8081:8080"  # Standard Airflow port
    depends_on:
      - airflow-metadata
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    networks:
      - airflow-net
    command: ["airflow", "webserver"]


  airflow-scheduler:
    build: .
    image: airflow-with-duckdb
    container_name: airflow-scheduler
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-metadata/airflow_metadata
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    depends_on:
      - airflow-metadata
      - airflow
    networks:
      - airflow-net
    command: ["airflow", "scheduler"]

  airflow-init:
    build: .
    image: airflow-init
    container_name: airflow-init
    command:
      - /bin/sh
      - -c
      - |
        airflow db init &&
        airflow users create \
          --username airflow \
          --firstname Airflow \
          --lastname User \
          --role Admin \
          --email airflow@example.com \
          --password airflow
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-metadata/airflow_metadata
    depends_on:
      - airflow-metadata
    networks:
      - airflow-net




  # Airflow Metadata Database Service
  airflow-metadata:
    image: postgres:15
    container_name: airflow_metadata_db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow_metadata
    ports:
      - "5433:5432"  # Expose on a different port to avoid conflict
    volumes:
      - airflow_metadata_data:/var/lib/postgresql/data
    networks:
      - airflow-net

  dbt:
    image: ghcr.io/dbt-labs/dbt-postgres:1.8.2
    container_name: dbt
    build:
      context: ./docker
      dockerfile: Dockerfile-dbt
    entrypoint: bash
    tty: true
    depends_on:
      - postgres
    ports:
      - 8060:8060
    volumes:
      - ./dbt/project/dbt_project:/usr/app
      - ./dbt/.dbt:/root/.dbt
      - ./data:/app/data
    networks:
      - airflow-net

  # Application Database Service
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: raw_data
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_script:/docker-entrypoint-initdb.d
    networks:
      - airflow-net

  # IDE for accessing Postgres
  pgadmin:
    container_name: pgadmin
    image: elestio/pgadmin:REL-8_10
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: root
    volumes:
      - ./data/pgadmin_data:/var/lib/pgadmin
    ports:
      - 5050:80
    networks:
      - airflow-net

  oracle:
    image: oracleinanutshell/oracle-xe-11g
    container_name: oracle
    environment:
      ORACLE_PASSWORD: oracle
    ports:
      - "1521:1521"
    volumes:
      - oracle_data:/u01/app/oracle
    networks:
      - airflow-net

  duckdb:
    image: python:3.9-slim
    container_name: duckdb
    build:
      context: ./docker
      dockerfile: Dockerfile-duckdb-iceberg
    volumes:
      - ./data:/app/data
    stdin_open: true
    tty: true
    networks:
      - airflow-net
    environment:
      - DUCKDB_DATABASE_PATH=/app/data/my_database.duckdb


  minio:
    image: minio/minio:RELEASE.2024-10-13T13-34-11Z
    container_name: minio
    ports:
      - "9000:9000"   # MinIO API port
      - "9001:9001"   # MinIO Console port
    env_file: ./docker/.env
    command: server /data --console-address ":9001"
    volumes:
      - ./data/minio_data:/data
    networks:
      airflow-net:
        aliases:
          - warehouse.minio

  iceberg_rest:
    image: tabulario/iceberg-rest:1.6.0
    container_name: iceberg_rest
    ports:
      - "8181:8181" 
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000 
    depends_on:
      - minio
    networks:
      airflow-net:

  mc:
    depends_on:
      - minio
    image: minio/mc:RELEASE.2024-10-02T08-27-28Z
    container_name: mc
    networks:
      airflow-net:
    entrypoint: >
        /bin/sh -c "
        until (/usr/bin/mc config host add minio http://minio:9000 minioadmin minioadmin) do echo '...waiting...' && sleep 1; done;
        /usr/bin/mc rm -r --force minio/warehouse;
        /usr/bin/mc mb minio/warehouse;
        /usr/bin/mc policy set public minio/warehouse;
        "

  neo4j:
    image: neo4j:5
    container_name: neo4j
    environment:
      - NEO4J_AUTH=neo4j/neo4jpassword  # Replace "neo4jpassword" with your desired password
    ports:
      - "7474:7474"  # Neo4j Web UI
      - "7687:7687"  # Bolt Protocol (for database connections)
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
      - ./data:/app/data
    networks:
      - airflow-net


  sl:
    container_name: sl
    build:
      context: ./docker  # Correct folder containing your Dockerfile
      dockerfile: Dockerfile-streamlit
    volumes:
      - ./streamlit_app:/app  # Ensure this matches the folder containing `app.py`
      - ./data:/app/data
    ports:
      - "8501:8501"  # Default Streamlit port
      - "8502:8502"  # Alternative port for additional instances (if needed)
    networks:
      - airflow-net



volumes:
  postgres_data:
  oracle_data:
  airflow_metadata_data:
  neo4j_data:
  neo4j_logs:
  neo4j_import:

networks:
  airflow-net:
    driver: bridge